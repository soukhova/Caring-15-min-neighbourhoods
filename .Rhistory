end.time <- Sys.time()
print(paste0("OD matrix calculation took ", round(difftime(end.time, start.time, units = "mins"), digits = 2), " minutes..."))
#"OD matrix calculation took 6.23 minutes..."
ttm_care_TRANSIT_parcel4 <- as.data.frame(output_df)
save("ttm_care_TRANSIT_parcel4", file = "parcel-ttm/ttm_care_TRANSIT_parcel4.rda")
rm("ttm_care_TRANSIT_parcel4")
library(AppliedPredictiveModeling) # Functions and Data Sets for 'Applied Predictive Modeling'
library(caret) # Classification and Regression Training
library(cowplot) # Streamlined Plot Theme and Plot Annotations for 'ggplot2'
library(ggparty) # 'ggplot' Visualizations for the 'partykit' Package
library(party) # A Laboratory for Recursive Partytioning
library(plotly) # Create Interactive Web Graphics via 'plotly.js'
library(rpart) # Recursive Partitioning and Regression Trees
library(rpart.plot) # Plot 'rpart' Models: An Enhanced Version of 'plot.rpart'
library(skimr) # Compact and Flexible Summaries of Data
library(sf) # Simple Features for R
library(SOMbrero) # SOM Bound to Realize Euclidean and Relational Outputs
library(tidyverse) # Easily Install and Load the 'Tidyverse'
library(tmap)
library(here)
library(parsnip)
load("access_15.rda")
load("HAM_census_21.rda")
Community_Boundaries <- st_read("data-raw/boundaries/Community_Boundaries.shp") |> mutate(COMMUNITY_ = ifelse(COMMUNITY_ == "Hamilton", "Hamilton-Central", COMMUNITY_))|> st_transform(crs=4326)
City_Boundary <- st_read("data-raw/boundaries/City_Boundary.shp") |> st_transform(crs=4326)
hydro_p_LakeOntario <- st_read("data-raw/boundaries/hydro_p_LakeOntario.shp") |> st_transform(crs=4326)
ham_bay <- st_read("data-raw/boundaries/Waterbodies.shp") |> st_transform(crs=4326) |> filter(FEATURE_TY == "Lake")
ham_bay_cropped<-st_crop(ham_bay$geometry, Community_Boundaries$geometry)
hydro_p_LakeOntario_cropped<-st_crop(hydro_p_LakeOntario$geometry, Community_Boundaries$geometry)
access_15_aggregated <- access_15 |>
group_by(from_id,
Care_Category) |>
summarize(GeoUID = first(GeoUID),
copp_isc = sum(copp_isc),
.groups = "drop") |>
left_join(HAM_census_21 |> # and dssociate with census data and drop parcels in DAs with 0 population:
st_drop_geometry(),
by = "GeoUID")
access_15_aggregated <- access_15_aggregated |> filter(Population >= 0 | !is.na(Population))
access_15_aggregated_total <- access_15_aggregated |>
group_by(from_id) |>
summarize(tot_copp_isc = sum(copp_isc),
.groups = "drop")
summary(access_15_aggregated_total)
access_15_aggregated_total |>
filter(tot_copp_isc == 0) |>
nrow()
access_15_aggregated <- access_15_aggregated |>
left_join(access_15_aggregated_total,
by = "from_id")
access_15_aggregated <- access_15_aggregated |>
mutate(prop_copp_isc = ifelse(tot_copp_isc > 0, copp_isc/tot_copp_isc, 0))
access_15_aggregated |>
group_by(from_id) |>
summarize(prop = sum(prop_copp_isc)) |>
filter(round(prop) != 1)
access_15_aggregated <- access_15_aggregated |>
mutate(copp_isc_norm = ifelse(tot_copp_isc != 0, (copp_isc - min(tot_copp_isc))/(max(tot_copp_isc) - min(tot_copp_isc)), 0))
access_15_agg_div <- access_15_aggregated |>
group_by(from_id) |>
summarize(div = -sum(prop_copp_isc * log(prop_copp_isc), na.rm = TRUE)/log(4))
access_15_prop_wide <- access_15_aggregated |>
select(from_id, Care_Category, prop_copp_isc) |>
pivot_wider(names_from = "Care_Category",
values_from = "prop_copp_isc") |>
rename_with(-from_id,
.fn = ~paste0("prop_",
str_replace(.x, "-","_")))
access_15_norm_wide <- access_15_aggregated |>
select(from_id, Care_Category, copp_isc_norm) |>
pivot_wider(names_from = "Care_Category",
values_from = "copp_isc_norm") |>
rename_with(-from_id,
.fn = ~paste0("norm_", str_replace(.x, "-","_")))
access_15_total_wide <- access_15_aggregated |>
select(from_id, Care_Category, copp_isc) |>
pivot_wider(names_from = "Care_Category",
values_from = "copp_isc") |>
rename_with(-from_id,
.fn = ~paste0("total_", str_replace(.x, "-","_")))
summary(access_15_total_wide)
summary(access_15_norm_wide)
summary(access_15_prop_wide)
summary(access_15_agg_div)
access_15_wide <- access_15_total_wide |>
left_join(access_15_norm_wide,
by = "from_id") |>
left_join(access_15_prop_wide,
by = "from_id") |>
left_join(access_15_agg_div,
by = "from_id")
summary(access_15_wide)
access_15_DA <- left_join(access_15_wide,
access_15_aggregated |> group_by(from_id) |>
summarise(GeoUID = first(GeoUID),
tot_copp_isc = first(tot_copp_isc),
Population = first(Population),
LICO = first(`v_CA21_1085: Prevalence of low income based on the Low-income cut-offs, after tax (LICO-AT) (%)`),
Income_bottom_n = first( `v_CA21_1103: In bottom half of the distribution`),
Income_top_n = first(`v_CA21_1121: In top half of the distribution`),
Age_0to14_n = first(`v_CA21_11: 0 to 14 years`),
Age_15to64_n = first(`v_CA21_68: 15 to 64 years`),
Age_65plus_n = first(`v_CA21_251: 65 years and over`),
One_parent_families_n = first(`v_CA21_507: Total one-parent families`),
Couple_families_n = first(`v_CA21_500: Total couple families`)),
by=c("from_id")) |>
group_by(GeoUID)|>
summarise(Population = first(Population),
parcel_count = n(),
LICO = first(LICO),
Income_bottom_n = first(Income_bottom_n),
Income_top_n = first(Income_top_n),
Age_0to14_n = first(Age_0to14_n),
Age_15to64_n = first(Age_15to64_n),
Age_65plus_n = first(Age_65plus_n),
One_parent_families_n = first(One_parent_families_n),
Couple_families_n = first(Couple_families_n),
med_tot_copp_isc = median(tot_copp_isc),
med_div = median(div),
med_dep_copp = median(total_Dependent_centric),
med_err_copp = median(total_Errand_centric),
med_groc_copp = median(total_Grocery_centric),
med_health_copp = median(total_Health_centric)) |>
left_join(HAM_census_21 |> select(c(GeoUID)), by=c("GeoUID")) |> st_as_sf() |> st_make_valid()
access_15_DA <- access_15_DA |> mutate(Populationparcel = Population/parcel_count,
Income_bottom = Income_bottom_n / (Income_bottom_n + Income_top_n),
Age_0to14 = Age_0to14_n / (Age_0to14_n + Age_15to64_n + Age_65plus_n),
One_parent_families = One_parent_families_n / (One_parent_families_n + Couple_families_n))
access_15_DA$parcel_count |> sum() #a check
# very caring (mean 4 to 18.6; total p50 46) - very complete (div=0.9; p50 0.90) (Cluster 2)
# caring (mean 2.1 to 9; total p50 18) -  complete (div=0.81; p50 0.82) (Cluster 1)
# somewhat caring (0.2 to 6; total p50 13) - complete (div=0.71; p50 0.74) (Cluster 4)
# somewhat caring (0.2 to 6; total p50 7) - not complete (div=0.36; p50 0.40) (Cluster 3)
# not caring (0 to 4.7; total p50 3) - not complete (div=0.02; p50 0) (Cluster 5)
set.seed(5935)
# run the SOM algorithm
nrows <- 5
ncols <- 5
access.som <- trainSOM(x.data = access_15_wide |>
# Remove the non-normalized accessibility scores
select(-c(from_id, starts_with("total_"))),
dimension=c(nrows, ncols),
nb.save = 100,
radius.type = "letremy",
verbose = TRUE)
class(access.som)
summary(access.som)
plot(access.som,
what = "energy")
table(access.som$clustering)
plot(access.som,
what = "obs",
type = "hitmap")
par(mfrow=c(3,2))
plot(access.som,
what = "obs",
type = "color",
variable = 1,
show.names=TRUE,
main="norm_Dependent_centric")
plot(access.som,
what = "obs",
type = "color",
variable = 2,
show.names = TRUE,
main = "norm_Errand_centric")
plot(access.som,
what = "obs",
type = "color",
variable = 3,
show.names = TRUE,
main="norm_Grocery_centric")
plot(access.som,
what="obs",
type="color",
variable=4,
show.names = TRUE,
main="norm_Health_centric")
plot(access.som,
what="obs",
type="color",
variable = 9,
show.names = TRUE,
main="Diversity")
plot(access.som,
what = "obs",
type="barplot",
show.names=TRUE)
plot(access.som,
what="obs",
type="meanline",
key.loc=c(-2,8),
mar=c(0,2,2,0))
plot(access.som,
what="obs",
type="boxplot")
access.sc <- superClass(access.som,
k = 5)
summary(access.sc)
access_15_wide$cluster <- access.som$clustering
access_15_wide$scluster <- access.sc$cluster[access.som$clustering]
# transparentTheme(trans = .4)
# featurePlot(x = data.frame(access_15_wide$div,
#                            access_15_wide$Dependent_centric_norm,
#                            access_15_wide$Errand_centric_norm,
#                            access_15_wide$Grocery_centric_norm,
#                            access_15_wide$Health_centric_norm),
#             y = as.factor(access_15_wide$scluster),
#             plot = "pairs",
#             ## Add a key at the top
#             auto.key = list(columns = 8))
access_15_wide |>
select(scluster, starts_with("total_"), div) |>
group_by(scluster) |>
skim()
df <- access_15_wide |>
select(-c(cluster,
starts_with("prop"),
starts_with("norm"))) |>
rename(Dependent = total_Dependent_centric,
Grocery = total_Grocery_centric,
Errand = total_Errand_centric,
Health = total_Health_centric) |>
mutate(scluster = factor(scluster),
div = round(div, digits = 3))
df <- df |>
mutate(Total_copp = Dependent + Grocery + Errand + Health)
classmod.som <- rpart(data = df,
formula = scluster ~ (Dependent + Grocery + Errand + Health + div))
# Predict the labels and add them to the dataset
predicted_labels <- predict(classmod.som, df) |> as.data.frame()
df$SC1 <- predicted_labels$`1`
df$SC2 <- predicted_labels$`2`
df$SC3 <- predicted_labels$`3`
df$SC4 <- predicted_labels$`4`
df$SC5 <- predicted_labels$`5`
# df <- df |> mutate(max_label_val = pmax(predicted_labels$`1`, predicted_labels$`2`, predicted_labels$`3`, predicted_labels$`4`, predicted_labels$`5`),
#                    max_label = case_when(max_label_val == predicted_labels$`1` ~ "SC1",
#                                          max_label_val == predicted_labels$`2` ~ "SC2",
#                                          max_label_val == predicted_labels$`3` ~ "SC3",
#                                          max_label_val == predicted_labels$`4` ~ "SC4",
#                                          max_label_val == predicted_labels$`5` ~ "SC5"))
df_DA <- df |> left_join(access_15 |> dplyr::select(c("from_id","GeoUID")), by = "from_id") |>
group_by(GeoUID) |>
summarise(
SC1 = median(SC1),
SC2 = median(SC2),
SC3 = median(SC3),
SC4 = median(SC4),
SC5 = median(SC5),
max_label_val = pmax(SC1, SC2, SC3, SC4, SC5),
med_tot_copp_isc = median(Total_copp),
med_div = median(div),
med_dep_copp = median(Dependent),
med_err_copp = median(Errand),
med_groc_copp = median(Grocery),
med_health_copp = median(Health)) |>
mutate(max_label = case_when(max_label_val == SC1 ~ "b. Caring - complete (SC1)",
max_label_val == SC2 ~ "a. Very caring - very complete (SC2)",
max_label_val == SC3 ~ "d. Somewhat caring - not complete (SC3)",
max_label_val == SC4 ~ "c. Somewhat caring - complete (SC4)",
max_label_val == SC5 ~ "d. not caring - not complete (SC5)")) |>
left_join(HAM_census_21 |> select(c(GeoUID)), by=c("GeoUID")) |> st_as_sf() |> st_make_valid()
total_copp_plot <- tm_shape(ham_bay, bbox=Community_Boundaries) + tm_polygons(col="skyblue", border.alpha = 0) +
tm_shape(hydro_p_LakeOntario, bbox=Community_Boundaries) + tm_polygons(col="skyblue", border.alpha = 0)+
tm_shape(df_DA) +
tm_polygons("med_tot_copp_isc",
border.alpha = 0.2,
style = "quantile", n=4,
palette = "Purples",
title = "Total access to care destinations",) +
tm_shape(Community_Boundaries) + tm_polygons(alpha=0)+
tm_scale_bar(position = c("left", "bottom"), breaks = c(0, 1, 5, 10, 15)) +
tm_compass(position = c("left", "top"), size=1.0)+
tm_layout(legend.bg.color = "white", legend.bg.alpha = 0.4,
bg.color = "grey",
legend.position = c("right","top"))
SClabel_copp_plot <- tm_shape(ham_bay, bbox=Community_Boundaries) + tm_polygons(col="skyblue", border.alpha = 0) +
tm_shape(hydro_p_LakeOntario, bbox=Community_Boundaries) + tm_polygons(col="skyblue", border.alpha = 0)+
tm_shape(df_DA) +
tm_polygons("max_label",
border.alpha = 0.2,
palette = c("#44ce1b","#bbdb44", "#f7e379", "#f2a134","#e51f1f"),
labels = (c("Very caring - very complete (SC2)","Caring - Complete (SC1)",
"Somewhat caring - complete (SC4)","Somewhat caring - somewhat complete (SC5)",
"Somewhat caring - somewhat complete (SC3)")),
title = "Prevelant supercluster") +
tm_shape(Community_Boundaries) + tm_polygons(alpha=0)+
tm_scale_bar(position = c("left", "bottom"), breaks = c(0, 1, 5, 10, 15)) +
tm_compass(position = c("left", "top"), size=1.0)+
tm_layout(legend.bg.color = "white", legend.bg.alpha = 0.4,
bg.color = "grey",
legend.position = c("right","top"))
SClabel_copp_plot
SCLab_and_total_copp_plot <- tmap_arrange(SClabel_copp_plot, total_copp_plot, nrow=2)
tmap_save(SCLab_and_total_copp_plot,
file= file.path(here("figures/SCLab_and_total_copp_plot.png")),
dpi=300)
library(disk.frame)
library(cancensus)
library(sf)
library(leaflet)
library(r5r)
library(dplyr)
library(purrr)
setup_disk.frame()
# increase Java memory
options(java.parameters = "-Xmx6G")
#changing my time zone to Toronto's time zone
Sys.setenv(TZ='EDT')
care_dest <- read.csv("data-raw/FINAL_Care_Destinations_2023.csv")
care_dest <- st_as_sf(care_dest, coords = c("LONGITUDE", "LATITUDE"))
load("HAM_census_21.rda")
Community_Boundaries <- read_sf("data-raw/boundaries/Community_Boundaries.shp") |> st_transform(st_crs(HAM_census_21))
City_Boundary <- read_sf("data-raw/boundaries/City_Boundary.shp") |> st_transform(st_crs(HAM_census_21))
# the r5r package requires Java Development Kit version 11, which can be downloaded from https://www.oracle.com/java/technologies/javase-jdk11-downloads.html . See the direction given in the r5r installation here (https://ipeagit.github.io/r5r/articles/r5r.html)
dir.create("./data-raw/tt") #create a folder, if it already exists - this function does nothing
r5_path <- file.path("./data-raw/tt")
list.files(r5_path)
# Specify the URL of the OSM file for Ontario - it takes a few minutes to download (~56MB)
download.file(url = "http://download.geofabrik.de/north-america/canada/ontario-latest.osm.pbf",
destfile= file.path(r5_path, "Hamilton.osm.pbf"), mode = "wb")
#takesa few minutes
r5_HAM <- setup_r5(data_path = r5_path,  verbose = TRUE, elevation = "NONE")
load( file="care_dest.rda")
dest <- care_dest
dest$lon <- st_coordinates(dest)[,1]
dest$lat <- st_coordinates(dest)[,2]
dest <- dest %>%
st_drop_geometry() %>%
transmute(ID = as.character(ID),
lon,
lat) %>%
rename("id" = "ID")
load( "data-raw/parcels/R_PARCELS_CENTS_2020.rda")
orig_sf <- R_PARCELS_CENTS_2020 |> st_as_sf()
rm(R_PARCELS_CENTS_2020,R_PARCELS_2020)
coords <- st_coordinates(orig_sf)
orig <- data.frame(id = orig_sf$ID,
lon = st_coordinates(orig_sf)[,1],
lat = st_coordinates(orig_sf)[,2])
rm(orig_sf)
# due to size, the calculation of travel times needs to be batched. We split the origins into 4 distinct dataframes.
orig_1 <- orig[1:36903,]
orig_2 <- orig[36904:73807,]
orig_3 <- orig[73808:110710,]
orig_4 <- orig[110711:143893,]
#check, make sure sums to 143893
nrow(orig_1)+nrow(orig_2)+nrow(orig_3)+nrow(orig_4)
# set up batching according to how many origin rows to process at one time
chunksize = 50
num_chunks = ceiling(nrow(orig_1)/chunksize)
# create origin-destination pairs
origins_chunks <- as.disk.frame(orig_1,
outdir = "parcel-ttm/processing/orig",
nchunks = num_chunks,
overwrite = TRUE)
start.time <- Sys.time()
pb <- txtProgressBar(0, num_chunks, style = 3)
for (i in 1:num_chunks){
Orig_chunk <- get_chunk(origins_chunks, i)
ttm_chunk <- travel_time_matrix(r5r_core = r5_HAM,
origins = Orig_chunk,
destinations = dest,
mode = c("TRANSIT","WALK"),
max_walk_time <- 15,
departure_datetime = as.POSIXct(strptime("08-06-2023 08:00:00", # 12 GMT is 8am in Toronto (EDT)
format = "%d-%m-%Y %H:%M:%S")),
max_trip_duration = 30)
# export output as disk.frame
ifelse(i == 1, output_df <- as.disk.frame(ttm_chunk,
nchunks = 1,
outdir = "parcel-ttm/processing/output_ttm",
compress = 50,
overwrite = TRUE),
add_chunk(output_df, ttm_chunk, chunk_id = i))
setTxtProgressBar(pb, i)
}
ttm_chunk <- travel_time_matrix(r5r_core = r5_HAM,
origins = Orig_chunk,
destinations = dest,
mode = c("TRANSIT","WALK"),
max_walk_time = 15,
departure_datetime = as.POSIXct(strptime("08-06-2023 08:00:00", # 12 GMT is 8am in Toronto (EDT)
format = "%d-%m-%Y %H:%M:%S")),
max_trip_duration = 30)
# create origin-destination pairs
origins_chunks <- as.disk.frame(orig_1,
outdir = "parcel-ttm/processing/orig",
nchunks = num_chunks,
overwrite = TRUE)
# create origin-destination pairs
origins_chunks <- as.disk.frame(orig_1,
outdir = "parcel-ttm/processing/orig",
nchunks = num_chunks,
overwrite = TRUE)
start.time <- Sys.time()
pb <- txtProgressBar(0, num_chunks, style = 3)
pb <- txtProgressBar(0, num_chunks, style = 3)
for (i in 1:num_chunks){
Orig_chunk <- get_chunk(origins_chunks, i)
ttm_chunk <- travel_time_matrix(r5r_core = r5_HAM,
origins = Orig_chunk,
destinations = dest,
mode = c("TRANSIT","WALK"),
max_walk_time = 15,
departure_datetime = as.POSIXct(strptime("08-06-2023 08:00:00", # 12 GMT is 8am in Toronto (EDT)
format = "%d-%m-%Y %H:%M:%S")),
max_trip_duration = 30)
# export output as disk.frame
ifelse(i == 1, output_df <- as.disk.frame(ttm_chunk,
nchunks = 1,
outdir = "parcel-ttm/processing/output_ttm",
compress = 50,
overwrite = TRUE),
add_chunk(output_df, ttm_chunk, chunk_id = i))
setTxtProgressBar(pb, i)
}
for (i in 1:num_chunks){
Orig_chunk <- get_chunk(origins_chunks, i)
ttm_chunk <- travel_time_matrix(r5r_core = r5_HAM,
origins = Orig_chunk,
destinations = dest,
mode = c("TRANSIT","WALK"),
max_walk_time = 15,
departure_datetime = as.POSIXct(strptime("08-06-2023 08:00:00", # 12 GMT is 8am in Toronto (EDT)
format = "%d-%m-%Y %H:%M:%S")),
max_trip_duration = 30)
# export output as disk.frame
ifelse(i == 1, output_df <- as.disk.frame(ttm_chunk,
nchunks = 1,
outdir = "parcel-ttm/processing/output_ttm",
compress = 50,
overwrite = TRUE),
add_chunk(output_df, ttm_chunk, chunk_id = i))
setTxtProgressBar(pb, i)
}
end.time <- Sys.time()
for (i in 1:num_chunks){
Orig_chunk <- get_chunk(origins_chunks, i)
ttm_chunk <- travel_time_matrix(r5r_core = r5_HAM,
origins = Orig_chunk,
destinations = dest,
mode = c("TRANSIT","WALK"),
max_walk_time = 15,
departure_datetime = as.POSIXct(strptime("08-06-2023 08:00:00", # 12 GMT is 8am in Toronto (EDT)
format = "%d-%m-%Y %H:%M:%S")),
max_trip_duration = 30)
# export output as disk.frame
ifelse(i == 1, output_df <- as.disk.frame(ttm_chunk,
nchunks = 1,
outdir = "parcel-ttm/processing/output_ttm",
compress = 50,
overwrite = TRUE),
add_chunk(output_df, ttm_chunk, chunk_id = i))
setTxtProgressBar(pb, i)
}
#"OD matrix calculation took 6.78 minutes..."
ttm_care_TRANSIT_parcel1 <- as.data.frame(output_df)
save("ttm_care_TRANSIT_parcel1", file = "parcel-ttm/ttm_care_TRANSIT_parcel1_1.rda")
load("parcel-ttm/ttm_care_TRANSIT_parcel1.rda")
load("parcel-ttm/ttm_care_TRANSIT_parcel1.rda")
load("parcel-ttm/ttm_care_TRANSIT_parcel1_1.rda")
load("parcel-ttm/ttm_care_TRANSIT_parcel1.rda")
# set up batching according to how many origin rows to process at one time
chunksize = 50
num_chunks = ceiling(nrow(orig_1)/chunksize)
# create origin-destination pairs
origins_chunks <- as.disk.frame(orig_1,
outdir = "parcel-ttm/processing/orig",
nchunks = num_chunks,
overwrite = TRUE)
start.time <- Sys.time()
pb <- txtProgressBar(0, num_chunks, style = 3)
for (i in 1:num_chunks){
Orig_chunk <- get_chunk(origins_chunks, i)
ttm_chunk <- travel_time_matrix(r5r_core = r5_HAM,
origins = Orig_chunk,
destinations = dest,
mode = c("TRANSIT","WALK"),
max_walk_time = 15,
departure_datetime = as.POSIXct(strptime("08-06-2023 08:00:00", # 12 GMT is 8am in Toronto (EDT)
format = "%d-%m-%Y %H:%M:%S")),
max_trip_duration = 30)
# export output as disk.frame
ifelse(i == 1, output_df <- as.disk.frame(ttm_chunk,
nchunks = 1,
outdir = "parcel-ttm/processing/output_ttm",
compress = 50,
overwrite = TRUE),
add_chunk(output_df, ttm_chunk, chunk_id = i))
setTxtProgressBar(pb, i)
}
end.time <- Sys.time()
print(paste0("OD matrix calculation took ", round(difftime(end.time, start.time, units = "mins"), digits = 2), " minutes..."))
#"OD matrix calculation took 6.78 minutes..."
ttm_care_TRANSIT_parcel1 <- as.data.frame(output_df)
save("ttm_care_TRANSIT_parcel1", file = "parcel-ttm/ttm_care_TRANSIT_parcel1_1.rda")
rm("ttm_care_TRANSIT_parcel1_1")
#"OD matrix calculation took 6.78 minutes..."
ttm_care_TRANSIT_parcel1_1 <- as.data.frame(output_df)
save("ttm_care_TRANSIT_parcel1_1", file = "parcel-ttm/ttm_care_TRANSIT_parcel1_1.rda")
load("parcel-ttm/ttm_care_TRANSIT_parcel1_1.rda")
load("parcel-ttm/ttm_care_TRANSIT_parcel1.rda")
ttm_care_TRANSIT_parcel1|> summary()
ttm_care_TRANSIT_parcel1_1 |> summary()
View(ttm_care_TRANSIT_parcel1_1)
View(ttm_care_TRANSIT_parcel1)
load("C:/Users/user/OneDrive - McMaster University/05. PhD Research/14_15min-care-access-typolo/data/parcel-ttm/ttm_care_TRANSIT_parcel1.rda")
load("C:/Users/user/OneDrive - McMaster University/05. PhD Research/14_15min-care-access-typolo/data/parcel-ttm/ttm_care_walk_parcel1.rda")
View(ttm_care_TRANSIT_parcel1)
View(ttm_care_walk_parcel1)
